{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.python.platform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "# A list of all emojis\n",
    "from emojiList import emoji\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec:\n",
    "    def __init__(self, tweet_file):\n",
    "        self.file = tweet_file\n",
    "\n",
    "    def preprocess_tweets(self):\n",
    "        \"\"\" Tokenises all tweets to get words\"\"\"\n",
    "\n",
    "        # Tokenize the sentences\n",
    "        raw_sentences = []\n",
    "        tweets = open(self.file, \"r\")\n",
    "        for tweet in tweets:\n",
    "            raw_sentences.append(nltk.word_tokenize(tweet))\n",
    "        self.sentences = raw_sentences\n",
    "\n",
    "\n",
    "    def make_model(self):\n",
    "        \"\"\" Model and train the word2vec model on words from tweets\"\"\"\n",
    "\n",
    "        # Define parameters for the w2v model\n",
    "        num_features = 300\n",
    "        min_word_count = 3\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "        context_size = 7\n",
    "        downsampling = 1e-3\n",
    "        seed = 1\n",
    "\n",
    "        # Build the model\n",
    "        self.tweet2vec = w2v(\n",
    "            sg = 1,\n",
    "            seed = seed,\n",
    "            workers = num_workers,\n",
    "            size = num_features,\n",
    "            min_count = min_word_count,\n",
    "            window = context_size,\n",
    "            sample = downsampling\n",
    "        )\n",
    "\n",
    "        # Build the vocabulary\n",
    "        self.tweet2vec.build_vocab(self.sentences)\n",
    "        # Train the model\n",
    "        self.tweet2vec.train(self.sentences, epochs = 10, total_examples = len(self.sentences))\n",
    "\n",
    "    def run(self):\n",
    "        self.preprocess_tweets()\n",
    "        self.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getEmojis:\n",
    "    \n",
    "    \"\"\"Class to get Emojis from tweets\"\"\"\n",
    "    def __init__(self, tweet_file):\n",
    "        self.file = tweet_file\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"Reads all the tweets from the json file\"\"\"\n",
    "        with open(self.file, encoding='utf-8') as data_file:\n",
    "            self.data = json.loads(data_file.read())\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_emojis(s):\n",
    "        \"\"\"Given a tweet, returns emjois in it\"\"\"\n",
    "        return ' '.join(c for c in s if c in emoji)\n",
    "\n",
    "    def find_emojis(self):\n",
    "        \"\"\"Find all the emojis in the given data and unique emojis\"\"\"\n",
    "        # Extract the emoji from each tweet and save the unique emoji\n",
    "        # There is only one unique emoji per tweet\n",
    "        self.emoji_labels = []\n",
    "        for i, d in enumerate(self.data):\n",
    "            if i > 20000:\n",
    "                break\n",
    "            emoji_label = self.extract_emojis(d)\n",
    "            li = np.asarray(list(emoji_label.split(\" \")))\n",
    "            self.emoji_labels.append(np.unique(li))\n",
    "\n",
    "        self.unique_emojis = np.unique(self.emoji_labels)\n",
    "        self.unique_emojis = (np.array(self.unique_emojis.tolist())[1:]).tolist()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Helper function to run all required functions\"\"\"\n",
    "        self.read_data()\n",
    "        self.find_emojis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, labels, tweet2vec):\n",
    "        self.labels = labels\n",
    "        self.tweet2vec = tweet2vec\n",
    "\n",
    "    def make_data(self):\n",
    "        en_stopwords = set(stopwords.words('english'))\n",
    "        snowball_stemmer = SnowballStemmer('english')\n",
    "        \n",
    "        # Average w2v of every tweet to be used as a feature\n",
    "        features = []\n",
    "        tweets = open(\"tweet_file\", \"r\")\n",
    "        for counter, tweet in enumerate(tweets):\n",
    "            if counter > 20000:\n",
    "                break\n",
    "#             without_stopwords = [w for w in tweet.split() if w not in en_stopwords]\n",
    "#             stemmed = [snowball_stemmer.stem(w) for w in without_stopwords]\n",
    "#             stemmed = ' '.join(stemmed)\n",
    "            avg_vec = np.zeros(self.tweet2vec.wv.vector_size)\n",
    "            for word in tweet:\n",
    "                if word not in self.tweet2vec.wv.vocab or word in emoji:\n",
    "                    continue\n",
    "                avg_vec = np.add(avg_vec, self.tweet2vec.wv[word])\n",
    "            features.append(np.true_divide(avg_vec, len(tweet)))\n",
    "        self.features = np.asarray(features)\n",
    "        tweets.close()\n",
    "\n",
    "        # One hot encode the labels\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_labels = encoder.fit_transform(self.labels)\n",
    "        # Reshaping into a 2D vector\n",
    "#         encoded_labels = np.reshape(encoded_labels, (-1, encoded_labels.shape[0]))\n",
    "#         print(encoded_labels)\n",
    "\n",
    "#         encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.one_hot = self.one_hot_labelling(encoded_labels)\n",
    "\n",
    "    def one_hot_labelling(self, encoded_labels):\n",
    "        hot = np.zeros((encoded_labels.size, encoded_labels.max()+1))\n",
    "        hot[np.arange(encoded_labels.size), encoded_labels] = 1\n",
    "        return hot\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        num_epochs = 100\n",
    "        learningRate = 0.3\n",
    "        num_labels = np.unique(self.labels).shape[0]\n",
    "        num_features = self.features.shape[1]\n",
    "        x = tf.placeholder(\"float\", shape=[None, num_features])\n",
    "        W = tf.Variable(tf.zeros([num_features, num_labels]))\n",
    "        b = tf.Variable(tf.zeros([num_labels]))\n",
    "        y_ = tf.placeholder(\"float\", shape=[None, num_labels])\n",
    "        features, labels = shuffle(self.features, self.one_hot, random_state = 1)\n",
    "        # Split the data into testing and training sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features, labels, test_size=0.2, random_state=42)\n",
    "        print(train_features.shape)\n",
    "        print(train_labels.shape)\n",
    "        print(num_labels)\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.truncated_normal([num_features, 60])),\n",
    "            'h2': tf.Variable(tf.truncated_normal([60, 60])),\n",
    "            'h3': tf.Variable(tf.truncated_normal([60, 60])),\n",
    "            'h4': tf.Variable(tf.truncated_normal([60, 60])),\n",
    "            'out': tf.Variable(tf.truncated_normal([60, num_labels]))\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.truncated_normal([60])),\n",
    "            'b2': tf.Variable(tf.truncated_normal([60])),\n",
    "            'b3': tf.Variable(tf.truncated_normal([60])),\n",
    "            'b4': tf.Variable(tf.truncated_normal([60])),\n",
    "            'out': tf.Variable(tf.truncated_normal([num_labels]))\n",
    "        }\n",
    "\n",
    "        model_path = \"./model\"\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        y = self.multilayer_perceptron(x, weights, biases)\n",
    "        # y = _generateTensorLayers(x, weights, biases)\n",
    "\n",
    "        cost_function = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "        training_step = tf.train.GradientDescentOptimizer(\n",
    "            learningRate).minimize(cost_function)\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        mse_history = []\n",
    "        accuracy_history = []\n",
    "        cost_history = np.empty(shape = [1], dtype=float)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            sess.run(training_step, feed_dict={\n",
    "                     x: train_features, y_: train_labels})\n",
    "            cost = sess.run(cost_function, feed_dict={\n",
    "                            x: train_features, y_: train_labels})\n",
    "            cost_history = np.append(cost_history, cost)\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "            # tf.Print(y, [y])\n",
    "            # sess.run(y, feed_dict={x: train_features})\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            pred_y = sess.run(y, feed_dict={x: test_features})\n",
    "            mse = tf.reduce_mean(tf.square(pred_y - test_labels))\n",
    "            mse_history.append(sess.run(mse))\n",
    "            accuracy = (sess.run(accuracy, feed_dict={\n",
    "                        x: train_features, y_: train_labels}))\n",
    "            accuracy_history.append(accuracy)\n",
    "            print('epoch: ', epoch, ' - cost: ', cost,\n",
    "                  \" - MSE: \", mse, \"- Train Accuracy: \", accuracy)\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        print(\" Model Saved in file: {}\".format(save_path))\n",
    "        pred_y = sess.run(y, feed_dict={x: test_features})\n",
    "        mse = tf.reduce_mean(tf.square(pred_y - test_labels))\n",
    "        print(\"MSE : {}\".format(sess.run(mse)))\n",
    "\n",
    "    @staticmethod\n",
    "    def multilayer_perceptron(x, weights, biases):\n",
    "\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "        layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "        out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "        print (\"Hello\")\n",
    "        return out_layer\n",
    "\n",
    "    def run(self):\n",
    "        self.make_data()\n",
    "        self.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = getEmojis(\"resultdata.json\")\n",
    "w2vec = word2vec(\"train.txt.text\")\n",
    "# em.run()\n",
    "w2vec.run()\n",
    "# labels = em.emoji_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samyak/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 300)\n",
      "(16000, 20)\n",
      "20\n",
      "WARNING:tensorflow:From /home/samyak/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Hello\n",
      "WARNING:tensorflow:From <ipython-input-9-2a64a94c22dd>:86: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch:  0  - cost:  65.73263  - MSE:  Tensor(\"Mean_2:0\", shape=(), dtype=float64) - Train Accuracy:  0.0253125\n",
      "epoch:  1  - cost:  102.848656  - MSE:  Tensor(\"Mean_4:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  2  - cost:  135.37259  - MSE:  Tensor(\"Mean_6:0\", shape=(), dtype=float64) - Train Accuracy:  0.104125\n",
      "epoch:  3  - cost:  26.358427  - MSE:  Tensor(\"Mean_8:0\", shape=(), dtype=float64) - Train Accuracy:  0.1011875\n",
      "epoch:  4  - cost:  11.80418  - MSE:  Tensor(\"Mean_10:0\", shape=(), dtype=float64) - Train Accuracy:  0.0446875\n",
      "epoch:  5  - cost:  4.1159706  - MSE:  Tensor(\"Mean_12:0\", shape=(), dtype=float64) - Train Accuracy:  0.0456875\n",
      "epoch:  6  - cost:  3.29377  - MSE:  Tensor(\"Mean_14:0\", shape=(), dtype=float64) - Train Accuracy:  0.0474375\n",
      "epoch:  7  - cost:  3.2531197  - MSE:  Tensor(\"Mean_16:0\", shape=(), dtype=float64) - Train Accuracy:  0.1075\n",
      "epoch:  8  - cost:  3.429681  - MSE:  Tensor(\"Mean_18:0\", shape=(), dtype=float64) - Train Accuracy:  0.0350625\n",
      "epoch:  9  - cost:  3.252724  - MSE:  Tensor(\"Mean_20:0\", shape=(), dtype=float64) - Train Accuracy:  0.046125\n",
      "epoch:  10  - cost:  3.2368088  - MSE:  Tensor(\"Mean_22:0\", shape=(), dtype=float64) - Train Accuracy:  0.046125\n",
      "epoch:  11  - cost:  3.2217293  - MSE:  Tensor(\"Mean_24:0\", shape=(), dtype=float64) - Train Accuracy:  0.046125\n",
      "epoch:  12  - cost:  3.2074442  - MSE:  Tensor(\"Mean_26:0\", shape=(), dtype=float64) - Train Accuracy:  0.046125\n",
      "epoch:  13  - cost:  3.1938462  - MSE:  Tensor(\"Mean_28:0\", shape=(), dtype=float64) - Train Accuracy:  0.0461875\n",
      "epoch:  14  - cost:  3.1808891  - MSE:  Tensor(\"Mean_30:0\", shape=(), dtype=float64) - Train Accuracy:  0.2146875\n",
      "epoch:  15  - cost:  3.1685362  - MSE:  Tensor(\"Mean_32:0\", shape=(), dtype=float64) - Train Accuracy:  0.2146875\n",
      "epoch:  16  - cost:  3.1567168  - MSE:  Tensor(\"Mean_34:0\", shape=(), dtype=float64) - Train Accuracy:  0.2146875\n",
      "epoch:  17  - cost:  3.1454253  - MSE:  Tensor(\"Mean_36:0\", shape=(), dtype=float64) - Train Accuracy:  0.2146875\n",
      "epoch:  18  - cost:  3.1345787  - MSE:  Tensor(\"Mean_38:0\", shape=(), dtype=float64) - Train Accuracy:  0.2146875\n",
      "epoch:  19  - cost:  3.1241455  - MSE:  Tensor(\"Mean_40:0\", shape=(), dtype=float64) - Train Accuracy:  0.2146875\n",
      "epoch:  20  - cost:  3.1141586  - MSE:  Tensor(\"Mean_42:0\", shape=(), dtype=float64) - Train Accuracy:  0.2146875\n",
      "epoch:  21  - cost:  3.104461  - MSE:  Tensor(\"Mean_44:0\", shape=(), dtype=float64) - Train Accuracy:  0.21475\n",
      "epoch:  22  - cost:  3.0951972  - MSE:  Tensor(\"Mean_46:0\", shape=(), dtype=float64) - Train Accuracy:  0.21475\n",
      "epoch:  23  - cost:  3.0862203  - MSE:  Tensor(\"Mean_48:0\", shape=(), dtype=float64) - Train Accuracy:  0.21475\n",
      "epoch:  24  - cost:  3.077501  - MSE:  Tensor(\"Mean_50:0\", shape=(), dtype=float64) - Train Accuracy:  0.21475\n",
      "epoch:  25  - cost:  3.0690854  - MSE:  Tensor(\"Mean_52:0\", shape=(), dtype=float64) - Train Accuracy:  0.21475\n",
      "epoch:  26  - cost:  3.0609694  - MSE:  Tensor(\"Mean_54:0\", shape=(), dtype=float64) - Train Accuracy:  0.2148125\n",
      "epoch:  27  - cost:  3.053074  - MSE:  Tensor(\"Mean_56:0\", shape=(), dtype=float64) - Train Accuracy:  0.2148125\n",
      "epoch:  28  - cost:  3.0453713  - MSE:  Tensor(\"Mean_58:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  29  - cost:  3.0379303  - MSE:  Tensor(\"Mean_60:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  30  - cost:  3.0306537  - MSE:  Tensor(\"Mean_62:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  31  - cost:  3.0236342  - MSE:  Tensor(\"Mean_64:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  32  - cost:  3.0168004  - MSE:  Tensor(\"Mean_66:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  33  - cost:  3.0100684  - MSE:  Tensor(\"Mean_68:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  34  - cost:  3.003576  - MSE:  Tensor(\"Mean_70:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  35  - cost:  2.9972262  - MSE:  Tensor(\"Mean_72:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  36  - cost:  2.991015  - MSE:  Tensor(\"Mean_74:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  37  - cost:  2.984936  - MSE:  Tensor(\"Mean_76:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  38  - cost:  2.979018  - MSE:  Tensor(\"Mean_78:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  39  - cost:  2.97326  - MSE:  Tensor(\"Mean_80:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  40  - cost:  2.967635  - MSE:  Tensor(\"Mean_82:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  41  - cost:  2.962079  - MSE:  Tensor(\"Mean_84:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  42  - cost:  2.9566963  - MSE:  Tensor(\"Mean_86:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  43  - cost:  2.951403  - MSE:  Tensor(\"Mean_88:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  44  - cost:  2.946247  - MSE:  Tensor(\"Mean_90:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  45  - cost:  2.9411724  - MSE:  Tensor(\"Mean_92:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  46  - cost:  2.9362128  - MSE:  Tensor(\"Mean_94:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  47  - cost:  2.9313922  - MSE:  Tensor(\"Mean_96:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  48  - cost:  2.9266336  - MSE:  Tensor(\"Mean_98:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  49  - cost:  2.9219906  - MSE:  Tensor(\"Mean_100:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  50  - cost:  2.917463  - MSE:  Tensor(\"Mean_102:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  51  - cost:  2.9130137  - MSE:  Tensor(\"Mean_104:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  52  - cost:  2.9086723  - MSE:  Tensor(\"Mean_106:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  53  - cost:  2.9043937  - MSE:  Tensor(\"Mean_108:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  54  - cost:  2.9002383  - MSE:  Tensor(\"Mean_110:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  55  - cost:  2.8961387  - MSE:  Tensor(\"Mean_112:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  56  - cost:  2.8921452  - MSE:  Tensor(\"Mean_114:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  57  - cost:  2.8882365  - MSE:  Tensor(\"Mean_116:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  58  - cost:  2.884398  - MSE:  Tensor(\"Mean_118:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  59  - cost:  2.8806503  - MSE:  Tensor(\"Mean_120:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  60  - cost:  2.8769722  - MSE:  Tensor(\"Mean_122:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  61  - cost:  2.873401  - MSE:  Tensor(\"Mean_124:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  62  - cost:  2.869887  - MSE:  Tensor(\"Mean_126:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  63  - cost:  2.866437  - MSE:  Tensor(\"Mean_128:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  64  - cost:  2.8630698  - MSE:  Tensor(\"Mean_130:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  65  - cost:  2.859777  - MSE:  Tensor(\"Mean_132:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  66  - cost:  2.8565748  - MSE:  Tensor(\"Mean_134:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  67  - cost:  2.8534403  - MSE:  Tensor(\"Mean_136:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  68  - cost:  2.8503525  - MSE:  Tensor(\"Mean_138:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  69  - cost:  2.8473477  - MSE:  Tensor(\"Mean_140:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  70  - cost:  2.8444107  - MSE:  Tensor(\"Mean_142:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  71  - cost:  2.841547  - MSE:  Tensor(\"Mean_144:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  72  - cost:  2.8387105  - MSE:  Tensor(\"Mean_146:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  73  - cost:  2.8360074  - MSE:  Tensor(\"Mean_148:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  74  - cost:  2.8333135  - MSE:  Tensor(\"Mean_150:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  75  - cost:  2.8306894  - MSE:  Tensor(\"Mean_152:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  76  - cost:  2.82818  - MSE:  Tensor(\"Mean_154:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  77  - cost:  2.8257036  - MSE:  Tensor(\"Mean_156:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  78  - cost:  2.823255  - MSE:  Tensor(\"Mean_158:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  79  - cost:  2.8209164  - MSE:  Tensor(\"Mean_160:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  80  - cost:  2.818601  - MSE:  Tensor(\"Mean_162:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  81  - cost:  2.8163342  - MSE:  Tensor(\"Mean_164:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  82  - cost:  2.814141  - MSE:  Tensor(\"Mean_166:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  83  - cost:  2.8119948  - MSE:  Tensor(\"Mean_168:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  84  - cost:  2.8099258  - MSE:  Tensor(\"Mean_170:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  85  - cost:  2.8078818  - MSE:  Tensor(\"Mean_172:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  86  - cost:  2.8059044  - MSE:  Tensor(\"Mean_174:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  87  - cost:  2.8039732  - MSE:  Tensor(\"Mean_176:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  88  - cost:  2.8021188  - MSE:  Tensor(\"Mean_178:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  89  - cost:  2.8003032  - MSE:  Tensor(\"Mean_180:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  90  - cost:  2.7985237  - MSE:  Tensor(\"Mean_182:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  91  - cost:  2.796808  - MSE:  Tensor(\"Mean_184:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  92  - cost:  2.7951322  - MSE:  Tensor(\"Mean_186:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  93  - cost:  2.7934813  - MSE:  Tensor(\"Mean_188:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  94  - cost:  2.7919106  - MSE:  Tensor(\"Mean_190:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  95  - cost:  2.790367  - MSE:  Tensor(\"Mean_192:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  96  - cost:  2.7888563  - MSE:  Tensor(\"Mean_194:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  97  - cost:  2.7873716  - MSE:  Tensor(\"Mean_196:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  98  - cost:  2.7859623  - MSE:  Tensor(\"Mean_198:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      "epoch:  99  - cost:  2.7845745  - MSE:  Tensor(\"Mean_200:0\", shape=(), dtype=float64) - Train Accuracy:  0.2149375\n",
      " Model Saved in file: ./model\n",
      "MSE : 0.39908180846961255\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(labels, w2vec.tweet2vec)\n",
    "# print(mlp.labels)\n",
    "mlp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('train.txt.text')\n",
    "label = open('train.txt.labels')\n",
    "tweets = []\n",
    "labels = []\n",
    "for line in text:\n",
    "    tweets.append(line)\n",
    "for lab in label:\n",
    "    labels.append(lab)\n",
    "labels = list(map(int, labels))\n",
    "text.close()\n",
    "label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the next month keep pancakes away from me #XiHop #txstaxid @ Texas State Alpha Xi Delta \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets[21342]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "# print (type(features))\n",
    "for counter, tweet in enumerate(tweets):\n",
    "    avg_vec = np.zeros(w2vec.tweet2vec.wv.vector_size)\n",
    "    for word in tweet:\n",
    "        if word not in w2vec.tweet2vec.wv.vocab or word in emoji:\n",
    "            continue\n",
    "        avg_vec = np.add(avg_vec, w2vec.tweet2vec.wv[word])\n",
    "    features.append(np.true_divide(avg_vec, len(tweet)))\n",
    "features = np.asarray(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77360, 300)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "from sklearn import svm\n",
    "# encoder = LabelEncoder()\n",
    "# encoded_labels = encoder.fit_transform(labels)\n",
    "train_features, train_labels = shuffle(features, labels, random_state = 1)\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "test_features = []\n",
    "test_labels = []\n",
    "text = open('us_test.text')\n",
    "label = open('us_test.labels')\n",
    "for line in text:\n",
    "    test_features.append(line)\n",
    "for line in labels:\n",
    "    test_labels.append(line)\n",
    "test_labels = list(map(int, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "61888/61888 [==============================] - 2s 36us/step - loss: 2.7454 - acc: 0.2171\n",
      "Epoch 2/40\n",
      "61888/61888 [==============================] - 2s 34us/step - loss: 2.7194 - acc: 0.2181\n",
      "Epoch 3/40\n",
      "61888/61888 [==============================] - 2s 33us/step - loss: 2.6882 - acc: 0.2185\n",
      "Epoch 4/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.6298 - acc: 0.2306\n",
      "Epoch 5/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.5639 - acc: 0.2520\n",
      "Epoch 6/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.5312 - acc: 0.2549\n",
      "Epoch 7/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.5191 - acc: 0.2512\n",
      "Epoch 8/40\n",
      "61888/61888 [==============================] - 2s 35us/step - loss: 2.5146 - acc: 0.2473\n",
      "Epoch 9/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.4924 - acc: 0.2468\n",
      "Epoch 10/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.5689 - acc: 0.2355\n",
      "Epoch 11/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.5752 - acc: 0.2326\n",
      "Epoch 12/40\n",
      "61888/61888 [==============================] - 2s 33us/step - loss: 2.5550 - acc: 0.2376\n",
      "Epoch 13/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.6390 - acc: 0.2290\n",
      "Epoch 14/40\n",
      "61888/61888 [==============================] - 2s 33us/step - loss: 2.4636 - acc: 0.2704\n",
      "Epoch 15/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.3938 - acc: 0.2828\n",
      "Epoch 16/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.4526 - acc: 0.2641\n",
      "Epoch 17/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.3529 - acc: 0.2864\n",
      "Epoch 18/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.4094 - acc: 0.2730\n",
      "Epoch 19/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.3925 - acc: 0.2697\n",
      "Epoch 20/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.3204 - acc: 0.2969\n",
      "Epoch 21/40\n",
      "61888/61888 [==============================] - 2s 30us/step - loss: 2.4291 - acc: 0.2755\n",
      "Epoch 22/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.2983 - acc: 0.3007\n",
      "Epoch 23/40\n",
      "61888/61888 [==============================] - 2s 33us/step - loss: 2.6362 - acc: 0.2354\n",
      "Epoch 24/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.4928 - acc: 0.2507\n",
      "Epoch 25/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.3767 - acc: 0.2902\n",
      "Epoch 26/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.3321 - acc: 0.2953\n",
      "Epoch 27/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.3132 - acc: 0.2996\n",
      "Epoch 28/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.2958 - acc: 0.3029\n",
      "Epoch 29/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.2997 - acc: 0.3004\n",
      "Epoch 30/40\n",
      "61888/61888 [==============================] - 2s 31us/step - loss: 2.3293 - acc: 0.2909\n",
      "Epoch 31/40\n",
      "61888/61888 [==============================] - 2s 38us/step - loss: 2.2743 - acc: 0.3062\n",
      "Epoch 32/40\n",
      "61888/61888 [==============================] - 2s 37us/step - loss: 2.3987 - acc: 0.2777\n",
      "Epoch 33/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.2598 - acc: 0.3115\n",
      "Epoch 34/40\n",
      "61888/61888 [==============================] - 2s 35us/step - loss: 2.2515 - acc: 0.3130\n",
      "Epoch 35/40\n",
      "61888/61888 [==============================] - 2s 39us/step - loss: 2.2694 - acc: 0.3067\n",
      "Epoch 36/40\n",
      "61888/61888 [==============================] - 2s 34us/step - loss: 2.2445 - acc: 0.3141\n",
      "Epoch 37/40\n",
      "61888/61888 [==============================] - 2s 32us/step - loss: 2.4868 - acc: 0.2600\n",
      "Epoch 38/40\n",
      "61888/61888 [==============================] - 2s 36us/step - loss: 2.2498 - acc: 0.3150\n",
      "Epoch 39/40\n",
      "61888/61888 [==============================] - 2s 38us/step - loss: 2.2440 - acc: 0.3166\n",
      "Epoch 40/40\n",
      "61888/61888 [==============================] - 2s 33us/step - loss: 2.5515 - acc: 0.2470\n",
      "15472/15472 [==============================] - 0s 18us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(400, activation='relu', input_dim=300))\n",
    "model.add(Dropout(40.5))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "y_train = np_utils.to_categorical(train_labels, 20)\n",
    "y_test = np_utils.to_categorical(test_labels, 20)\n",
    "model.fit(train_features, y_train,\n",
    "          epochs=40,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(test_features, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=100)\n",
    "neigh.fit(train_features, train_labels)\n",
    "pred = neigh.predict(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc =accuracy_score(test_labels, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.234552740434333"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
