{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "import math\n",
    "# A list of all emojis\n",
    "from emojiList import emoji\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.wrappers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec:\n",
    "    def __init__(self, tweet_file):\n",
    "        self.file = tweet_file\n",
    "\n",
    "    def preprocess_tweets(self):\n",
    "        \"\"\" Tokenises all tweets to get words\"\"\"\n",
    "\n",
    "        raw_sentences = []\n",
    "        tweets = open(self.file, \"r\")\n",
    "        for tweet in tweets:\n",
    "            raw_sentences.append(nltk.word_tokenize(tweet))\n",
    "        self.sentences = raw_sentences\n",
    "\n",
    "\n",
    "    def make_model(self):\n",
    "        \"\"\" Model and train the word2vec model on words from tweets\"\"\"\n",
    "\n",
    "        # Define parameters for the w2v model\n",
    "        num_features = 100\n",
    "        min_word_count = 3\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "        context_size = 7\n",
    "        downsampling = 1e-3\n",
    "        seed = 1\n",
    "\n",
    "        # Build the model\n",
    "        self.tweet2vec = w2v(\n",
    "            sg = 1,\n",
    "            seed = seed,\n",
    "            workers = num_workers,\n",
    "            size = num_features,\n",
    "            min_count = min_word_count,\n",
    "            window = context_size,\n",
    "            sample = downsampling\n",
    "        )\n",
    "\n",
    "        # Build the vocabulary\n",
    "        self.tweet2vec.build_vocab(self.sentences)\n",
    "        # Train the model\n",
    "        self.tweet2vec.train(self.sentences, epochs = 10, total_examples = len(self.sentences))\n",
    "\n",
    "    def run(self):\n",
    "        self.preprocess_tweets()\n",
    "        self.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getEmojis:\n",
    "    \n",
    "    \"\"\"Class to get Emojis from tweets\"\"\"\n",
    "    def __init__(self, tweet_file):\n",
    "        self.file = tweet_file\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"Reads all the tweets from the json file\"\"\"\n",
    "        with open(self.file, encoding='utf-8') as data_file:\n",
    "            self.data = json.loads(data_file.read())\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_emojis(s):\n",
    "        \"\"\"Given a tweet, returns emjois in it\"\"\"\n",
    "        return ' '.join(c for c in s if c in emoji)\n",
    "\n",
    "    def find_emojis(self):\n",
    "        \"\"\"Find all the emojis in the given data and unique emojis\"\"\"\n",
    "        # Extract the emoji from each tweet and save the unique emoji\n",
    "        # There is only one unique emoji per tweet\n",
    "        self.emoji_labels = []\n",
    "        for i, d in enumerate(self.data):\n",
    "#             if i > 20000:\n",
    "#                 break\n",
    "            emoji_label = self.extract_emojis(d)\n",
    "            li = np.asarray(list(emoji_label.split(\" \")))\n",
    "            self.emoji_labels.append(np.unique(li))\n",
    "            \n",
    "        self.unique_emojis = np.unique(self.emoji_labels)\n",
    "        self.unique_emojis = (np.array(self.unique_emojis.tolist())[1:]).tolist()\n",
    "        le = LabelEncoder()\n",
    "        encoded_labels = le.fit_transform(self.emoji_labels)\n",
    "        \n",
    "        f = open('train.text.labels', 'w+')\n",
    "        for label in encoded_labels:\n",
    "            f.write(\"%d\\n\" % label)\n",
    "        f.close()\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Helper function to run all required functions\"\"\"\n",
    "        self.read_data()\n",
    "        self.find_emojis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = getEmojis(\"resultdata.json\")\n",
    "w2vec = word2vec(\"train.txt\")\n",
    "# em.run()\n",
    "w2vec.run()\n",
    "# labels = em.emoji_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('train.txt')\n",
    "label = open('train.labels')\n",
    "train_tweets = []\n",
    "train_labels = []\n",
    "for line in text:\n",
    "    train_tweets.append(line)\n",
    "for lab in label:\n",
    "    train_labels.append(lab)\n",
    "train_labels = list(map(int, train_labels))\n",
    "text.close()\n",
    "label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = []\n",
    "test_labels = []\n",
    "text = open('us_test.text')\n",
    "label = open('us_test.labels')\n",
    "for line in text:\n",
    "    test_tweets.append(line)\n",
    "for line in label:\n",
    "    test_labels.append(line)\n",
    "test_labels = list(map(int, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(li):\n",
    "    features = []\n",
    "    max_len = 0\n",
    "    for counter, tweet in enumerate(li):\n",
    "        avg_vec = np.zeros(w2vec.tweet2vec.wv.vector_size)\n",
    "        max_len = max(max_len, len(tweet))\n",
    "        for word in tweet:\n",
    "            if word not in w2vec.tweet2vec.wv.vocab or word in emoji:\n",
    "                continue\n",
    "            avg_vec = np.add(avg_vec, w2vec.tweet2vec.wv[word])\n",
    "        features.append(np.true_divide(avg_vec, len(tweet)))\n",
    "    return np.asarray(features), max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector representation for each tweet\n",
    "X_train, max_len_train = get_vector(train_tweets)\n",
    "X_test, max_len_test = get_vector(test_tweets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = train_tweets + test_tweets\n",
    "max_length = math.ceil(sum([len(s.split(\" \")) for s in all_tweets])/len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tweets):\n",
    "    #Translate tweets to sequence of numbers\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', split=\" \", lower=True)\n",
    "    tokenizer.fit_on_texts(tweets)\n",
    "    return tokenizer, tokenizer.texts_to_sequences(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_weight_matrix(vocab, raw_embedding):\n",
    "    # Create weight matrix from pre-trained embeddings\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        if word in raw_embedding:\n",
    "            weight_matrix[i] = raw_embedding[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "weight_matrix = populate_weight_matrix(vocab, w2vec.tweet2vec.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, encoded_docs = encode_docs(all_tweets)\n",
    "temp_train = pad_sequences(encoded_docs[:len(train_tweets)], maxlen=max_length, padding='post')\n",
    "temp_test = pad_sequences(encoded_docs[-len(test_tweets):], maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 77360 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "77360/77360 [==============================] - 653s 8ms/step - loss: 2.1213 - acc: 0.3653 - val_loss: 2.0368 - val_acc: 0.3897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f0948b518>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np_utils.to_categorical(train_labels, 20)\n",
    "y_test = np_utils.to_categorical(test_labels, 20)\n",
    "embedding_layer = Embedding(len(vocab) + 1, 100, weights=[weight_matrix], input_length=max_length, trainable=True, mask_zero=True)\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(embedding_layer)\n",
    "model_rnn.add(Bidirectional(LSTM(128, dropout=0.2, return_sequences=True)))\n",
    "model_rnn.add(Bidirectional(LSTM(128, dropout=0.2)))\n",
    "model_rnn.add(Dense(400, activation='relu', input_dim=256))\n",
    "model_rnn.add(Dense(200, activation='relu'))\n",
    "model_rnn.add(Dense(20, activation='softmax'))\n",
    "# model.add(Dense(20, activation='softmax'))\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.fit(temp_train, y_train, epochs=1, validation_data=(temp_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_rnn.predict_classes(temp_test)\n",
    "acc = accuracy_score(test_labels, pred)\n",
    "f1 = f1_score(test_labels, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38972"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38971999999999996"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 13, 100)           13255300  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 13, 256)           234496    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 400)               102800    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                4020      \n",
      "=================================================================\n",
      "Total params: 14,071,056\n",
      "Trainable params: 14,071,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "77360/77360 [==============================] - 2s 32us/step - loss: 0.1806 - acc: 0.9506\n",
      "Epoch 2/20\n",
      "77360/77360 [==============================] - 2s 27us/step - loss: 0.1708 - acc: 0.9531\n",
      "Epoch 3/20\n",
      "77360/77360 [==============================] - 2s 25us/step - loss: 0.1643 - acc: 0.9549\n",
      "Epoch 4/20\n",
      "77360/77360 [==============================] - 2s 25us/step - loss: 0.1601 - acc: 0.9561\n",
      "Epoch 5/20\n",
      "77360/77360 [==============================] - 2s 28us/step - loss: 0.1585 - acc: 0.9564\n",
      "Epoch 6/20\n",
      "77360/77360 [==============================] - 2s 27us/step - loss: 0.1563 - acc: 0.9570\n",
      "Epoch 7/20\n",
      "77360/77360 [==============================] - 2s 25us/step - loss: 0.1556 - acc: 0.9570\n",
      "Epoch 8/20\n",
      "77360/77360 [==============================] - 2s 26us/step - loss: 0.1548 - acc: 0.9572\n",
      "Epoch 9/20\n",
      "77360/77360 [==============================] - 2s 30us/step - loss: 0.1542 - acc: 0.9573\n",
      "Epoch 10/20\n",
      "77360/77360 [==============================] - 2s 23us/step - loss: 0.1549 - acc: 0.9571\n",
      "Epoch 11/20\n",
      "77360/77360 [==============================] - 2s 24us/step - loss: 0.1548 - acc: 0.9570\n",
      "Epoch 12/20\n",
      "77360/77360 [==============================] - 2s 30us/step - loss: 0.1538 - acc: 0.9572\n",
      "Epoch 13/20\n",
      "77360/77360 [==============================] - 2s 29us/step - loss: 0.1555 - acc: 0.9570\n",
      "Epoch 14/20\n",
      "77360/77360 [==============================] - 2s 25us/step - loss: 0.1544 - acc: 0.9573\n",
      "Epoch 15/20\n",
      "77360/77360 [==============================] - 2s 25us/step - loss: 0.1562 - acc: 0.9568\n",
      "Epoch 16/20\n",
      "77360/77360 [==============================] - 2s 29us/step - loss: 0.1560 - acc: 0.9566\n",
      "Epoch 17/20\n",
      "77360/77360 [==============================] - 2s 28us/step - loss: 0.1554 - acc: 0.9568\n",
      "Epoch 18/20\n",
      "77360/77360 [==============================] - 2s 25us/step - loss: 0.1568 - acc: 0.9563\n",
      "Epoch 19/20\n",
      "77360/77360 [==============================] - 2s 29us/step - loss: 0.1594 - acc: 0.9556\n",
      "Epoch 20/20\n",
      "77360/77360 [==============================] - 2s 31us/step - loss: 0.1534 - acc: 0.9573\n",
      "50000/50000 [==============================] - 1s 14us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(400, activation='relu', input_dim=100))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "y_train = np_utils.to_categorical(train_labels, 20)\n",
    "y_test = np_utils.to_categorical(test_labels, 20)\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict On Training Set\n",
    "pred = model.predict_classes(X_test)\n",
    "acc = accuracy_score(test_labels, pred)\n",
    "f1 = f1_score(test_labels, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31286"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31286"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
