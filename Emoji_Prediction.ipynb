{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.python.platform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "# A list of all emojis\n",
    "from emojiList import emoji\n",
    "from gensim.models import Word2Vec as w2v\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec:\n",
    "    def __init__(self, tweet_file):\n",
    "        self.file = tweet_file\n",
    "\n",
    "    def preprocess_tweets(self):\n",
    "        \"\"\" Tokenises all tweets to get words\"\"\"\n",
    "\n",
    "        # Tokenize the sentences\n",
    "        raw_sentences = []\n",
    "        tweets = open(self.file, \"r\")\n",
    "        for tweet in tweets:\n",
    "            raw_sentences.append(nltk.word_tokenize(tweet))\n",
    "        self.sentences = raw_sentences\n",
    "\n",
    "\n",
    "    def make_model(self):\n",
    "        \"\"\" Model and train the word2vec model on words from tweets\"\"\"\n",
    "\n",
    "        # Define parameters for the w2v model\n",
    "        num_features = 300\n",
    "        min_word_count = 3\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "        context_size = 7\n",
    "        downsampling = 1e-3\n",
    "        seed = 1\n",
    "\n",
    "        # Build the model\n",
    "        self.tweet2vec = w2v(\n",
    "            sg = 1,\n",
    "            seed = seed,\n",
    "            workers = num_workers,\n",
    "            size = num_features,\n",
    "            min_count = min_word_count,\n",
    "            window = context_size,\n",
    "            sample = downsampling\n",
    "        )\n",
    "\n",
    "        # Build the vocabulary\n",
    "        self.tweet2vec.build_vocab(self.sentences)\n",
    "        # Train the model\n",
    "        self.tweet2vec.train(self.sentences, epochs = 10, total_examples = len(self.sentences))\n",
    "\n",
    "    def run(self):\n",
    "        self.preprocess_tweets()\n",
    "        self.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getEmojis:\n",
    "    \n",
    "    \"\"\"Class to get Emojis from tweets\"\"\"\n",
    "    def __init__(self, tweet_file):\n",
    "        self.file = tweet_file\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"Reads all the tweets from the json file\"\"\"\n",
    "        with open(self.file, encoding='utf-8') as data_file:\n",
    "            self.data = json.loads(data_file.read())\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_emojis(s):\n",
    "        \"\"\"Given a tweet, returns emjois in it\"\"\"\n",
    "        return ' '.join(c for c in s if c in emoji)\n",
    "\n",
    "    def find_emojis(self):\n",
    "        \"\"\"Find all the emojis in the given data and unique emojis\"\"\"\n",
    "        # Extract the emoji from each tweet and save the unique emoji\n",
    "        # There is only one unique emoji per tweet\n",
    "        self.emoji_labels = []\n",
    "        for i, d in enumerate(self.data):\n",
    "            if i > 20000:\n",
    "                break\n",
    "            emoji_label = self.extract_emojis(d)\n",
    "            li = np.asarray(list(emoji_label.split(\" \")))\n",
    "            self.emoji_labels.append(np.unique(li))\n",
    "\n",
    "        self.unique_emojis = np.unique(self.emoji_labels)\n",
    "        self.unique_emojis = (np.array(self.unique_emojis.tolist())[1:]).tolist()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Helper function to run all required functions\"\"\"\n",
    "        self.read_data()\n",
    "        self.find_emojis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, tweets, labels, tweet2vec):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.tweet2vec = tweet2vec\n",
    "\n",
    "    def make_data(self):\n",
    "        # Average w2v of every tweet to be used as a feature\n",
    "        features = []\n",
    "        for counter, tweet in enumerate(self.tweets):\n",
    "            avg_vec = np.zeros(self.tweet2vec.wv.vector_size)\n",
    "            for word in tweet:\n",
    "                if word not in self.tweet2vec.wv.vocab or word in emoji:\n",
    "                    continue\n",
    "                avg_vec = np.add(avg_vec, self.tweet2vec.wv[word])\n",
    "            features.append(np.true_divide(avg_vec, len(tweet)))\n",
    "        self.features = np.asarray(features)\n",
    "\n",
    "#     def one_hot_labelling(self, encoded_labels):\n",
    "#         hot = np.zeros((encoded_labels.size, encoded_labels.max()+1))\n",
    "#         hot[np.arange(encoded_labels.size), encoded_labels] = 1\n",
    "#         return hot\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        num_epochs = 100\n",
    "        learningRate = 0.3\n",
    "        num_labels = np.unique(self.labels).shape[0]\n",
    "        num_features = self.features.shape[1]\n",
    "        x = tf.placeholder(\"float\", shape=[None, num_features])\n",
    "        W = tf.Variable(tf.zeros([num_features, num_labels]))\n",
    "        b = tf.Variable(tf.zeros([num_labels]))\n",
    "        y_ = tf.placeholder(\"float\", shape=[None, num_labels])\n",
    "        features, labels = shuffle(self.features, self.labels, random_state = 1)\n",
    "        # Split the data into testing and training sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            self.features, self.labels, test_size=0.2, random_state=42)\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.truncated_normal([num_features, 60])),\n",
    "            'h2': tf.Variable(tf.truncated_normal([60, 60])),\n",
    "            'h3': tf.Variable(tf.truncated_normal([60, 60])),\n",
    "            'h4': tf.Variable(tf.truncated_normal([60, 60])),\n",
    "            'out': tf.Variable(tf.truncated_normal([60, num_labels]))\n",
    "        }\n",
    "\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.truncated_normal([60])),\n",
    "            'b2': tf.Variable(tf.truncated_normal([60])),\n",
    "            'b3': tf.Variable(tf.truncated_normal([60])),\n",
    "            'b4': tf.Variable(tf.truncated_normal([60])),\n",
    "            'out': tf.Variable(tf.truncated_normal([num_labels]))\n",
    "        }\n",
    "\n",
    "        model_path = \"./model\"\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        y = self.multilayer_perceptron(x, weights, biases)\n",
    "        # y = _generateTensorLayers(x, weights, biases)\n",
    "\n",
    "        cost_function = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "        training_step = tf.train.GradientDescentOptimizer(\n",
    "            learningRate).minimize(cost_function)\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        mse_history = []\n",
    "        accuracy_history = []\n",
    "        cost_history = np.empty(shape = [1], dtype=float)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            sess.run(training_step, feed_dict={\n",
    "                     x: train_features, y_: train_labels})\n",
    "            cost = sess.run(cost_function, feed_dict={\n",
    "                            x: train_features, y_: train_labels})\n",
    "            cost_history = np.append(cost_history, cost)\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "            # tf.Print(y, [y])\n",
    "            # sess.run(y, feed_dict={x: train_features})\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            pred_y = sess.run(y, feed_dict={x: test_features})\n",
    "            mse = tf.reduce_mean(tf.square(pred_y - test_labels))\n",
    "            mse_history.append(sess.run(mse))\n",
    "            accuracy = (sess.run(accuracy, feed_dict={\n",
    "                        x: train_features, y_: train_labels}))\n",
    "            accuracy_history.append(accuracy)\n",
    "            print('epoch: ', epoch, ' - cost: ', cost,\n",
    "                  \" - MSE: \", mse, \"- Train Accuracy: \", accuracy)\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        print(\" Model Saved in file: {}\".format(save_path))\n",
    "        pred_y = sess.run(y, feed_dict={x: test_features})\n",
    "        mse = tf.reduce_mean(tf.square(pred_y - test_labels))\n",
    "        print(\"MSE : {}\".format(sess.run(mse)))\n",
    "\n",
    "    @staticmethod\n",
    "    def multilayer_perceptron(x, weights, biases):\n",
    "\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "        layer_4 = tf.nn.relu(layer_4)\n",
    "\n",
    "        out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "        print (\"Hello\")\n",
    "        return out_layer\n",
    "\n",
    "    def run(self):\n",
    "        self.make_data()\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = getEmojis(\"resultdata.json\")\n",
    "w2vec = word2vec(\"train.txt.text\")\n",
    "# em.run()\n",
    "w2vec.run()\n",
    "# labels = em.emoji_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('train.txt.text')\n",
    "label = open('train.txt.labels')\n",
    "tweets = []\n",
    "labels = []\n",
    "for line in text:\n",
    "    tweets.append(line)\n",
    "for lab in label:\n",
    "    labels.append(lab)\n",
    "labels = list(map(int, labels))\n",
    "text.close()\n",
    "label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "test_labels = []\n",
    "text = open('us_test.text')\n",
    "label = open('us_test.labels')\n",
    "for line in text:\n",
    "    test_features.append(line)\n",
    "for line in labels:\n",
    "    test_labels.append(line)\n",
    "test_labels = list(map(int, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (61888,) for Tensor 'Placeholder_5:0', which has shape '(?, 20)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-138c2eb3b561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet2vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(mlp.labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-819a871c5a2c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-819a871c5a2c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             sess.run(training_step, feed_dict={\n\u001b[0;32m---> 76\u001b[0;31m                      x: train_features, y_: train_labels})\n\u001b[0m\u001b[1;32m     77\u001b[0m             cost = sess.run(cost_function, feed_dict={\n\u001b[1;32m     78\u001b[0m                             x: train_features, y_: train_labels})\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1086\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1087\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (61888,) for Tensor 'Placeholder_5:0', which has shape '(?, 20)'"
     ]
    }
   ],
   "source": [
    "mlp = MLP(tweets, labels, w2vec.tweet2vec)\n",
    "# print(mlp.labels)\n",
    "mlp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "# print (type(features))\n",
    "for counter, tweet in enumerate(tweets):\n",
    "    avg_vec = np.zeros(w2vec.tweet2vec.wv.vector_size)\n",
    "    for word in tweet:\n",
    "        if word not in w2vec.tweet2vec.wv.vocab or word in emoji:\n",
    "            continue\n",
    "        avg_vec = np.add(avg_vec, w2vec.tweet2vec.wv[word])\n",
    "    features.append(np.true_divide(avg_vec, len(tweet)))\n",
    "features = np.asarray(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features, train_labels = shuffle(features, labels, random_state = 1)\n",
    "test_features = []\n",
    "test_labels = []\n",
    "text = open('us_test.text')\n",
    "label = open('us_test.labels')\n",
    "for line in text:\n",
    "    test_features.append(line)\n",
    "for line in label:\n",
    "    test_labels.append(line)\n",
    "test_labels = list(map(int, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features = []\n",
    "# print (type(features))\n",
    "for counter, tweet in enumerate(test_features):\n",
    "    avg_vec = np.zeros(w2vec.tweet2vec.wv.vector_size)\n",
    "    for word in tweet:\n",
    "        if word not in w2vec.tweet2vec.wv.vocab or word in emoji:\n",
    "            continue\n",
    "        avg_vec = np.add(avg_vec, w2vec.tweet2vec.wv[word])\n",
    "    t_features.append(np.true_divide(avg_vec, len(tweet)))\n",
    "t_features = np.asarray(t_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(t_features.shape)\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "77360/77360 [==============================] - 5s 58us/step - loss: 2.7340 - acc: 0.2168\n",
      "Epoch 2/20\n",
      "77360/77360 [==============================] - 4s 52us/step - loss: 2.6990 - acc: 0.2180\n",
      "Epoch 3/20\n",
      "77360/77360 [==============================] - 4s 51us/step - loss: 2.6156 - acc: 0.2377\n",
      "Epoch 4/20\n",
      "77360/77360 [==============================] - 5s 60us/step - loss: 2.5361 - acc: 0.2602\n",
      "Epoch 5/20\n",
      "77360/77360 [==============================] - 5s 59us/step - loss: 2.5009 - acc: 0.2626\n",
      "Epoch 6/20\n",
      "77360/77360 [==============================] - 4s 56us/step - loss: 2.4464 - acc: 0.2709\n",
      "Epoch 7/20\n",
      "77360/77360 [==============================] - 5s 60us/step - loss: 2.4450 - acc: 0.2695\n",
      "Epoch 8/20\n",
      "77360/77360 [==============================] - 5s 64us/step - loss: 2.4095 - acc: 0.2760\n",
      "Epoch 9/20\n",
      "77360/77360 [==============================] - 4s 56us/step - loss: 2.4975 - acc: 0.2537\n",
      "Epoch 10/20\n",
      "77360/77360 [==============================] - 5s 61us/step - loss: 2.4903 - acc: 0.2516\n",
      "Epoch 11/20\n",
      "77360/77360 [==============================] - 4s 55us/step - loss: 2.5107 - acc: 0.2567\n",
      "Epoch 12/20\n",
      "77360/77360 [==============================] - 5s 59us/step - loss: 2.3696 - acc: 0.2849\n",
      "Epoch 13/20\n",
      "77360/77360 [==============================] - 5s 58us/step - loss: 2.4512 - acc: 0.2671\n",
      "Epoch 14/20\n",
      "77360/77360 [==============================] - 5s 59us/step - loss: 2.3915 - acc: 0.2790\n",
      "Epoch 15/20\n",
      "77360/77360 [==============================] - 5s 59us/step - loss: 2.3457 - acc: 0.2943\n",
      "Epoch 16/20\n",
      "77360/77360 [==============================] - 4s 56us/step - loss: 2.4141 - acc: 0.2812\n",
      "Epoch 17/20\n",
      "77360/77360 [==============================] - 5s 60us/step - loss: 2.2972 - acc: 0.3067\n",
      "Epoch 18/20\n",
      "77360/77360 [==============================] - 4s 55us/step - loss: 2.5843 - acc: 0.2481\n",
      "Epoch 19/20\n",
      "77360/77360 [==============================] - 5s 60us/step - loss: 2.3225 - acc: 0.3024\n",
      "Epoch 20/20\n",
      "77360/77360 [==============================] - 4s 57us/step - loss: 2.2876 - acc: 0.3071\n",
      "50000/50000 [==============================] - 1s 23us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "model = Sequential()\n",
    "model.add(Dense(600, activation='relu', input_dim=300))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "y_train = np_utils.to_categorical(train_labels, 20)\n",
    "y_test = np_utils.to_categorical(test_labels, 20)\n",
    "model.fit(train_features, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(t_features, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=100)\n",
    "neigh.fit(train_features, train_labels)\n",
    "pred = neigh.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc =accuracy_score(test_labels, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6620470484924317, 0.216]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
